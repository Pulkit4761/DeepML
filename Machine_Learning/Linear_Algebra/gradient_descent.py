'''In this problem, you need to implement a single function that 
can perform three variants of gradient descent Stochastic Gradient Descent (SGD),
 Batch Gradient Descent, and Mini Batch Gradient Descent using Mean Squared Error (MSE)
as the loss function. The function will take an additional parameter to 
specify which variant to use. Note: Do not shuffle the data'''

import numpy as np

def gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):
	# Your code here
	pass
